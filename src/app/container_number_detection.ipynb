{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd071caa2c22f4e8bb7a372824d4ea354f32e50a59e2bbfdcfcd535f56ab45c6d5c",
   "display_name": "Python 3.8.5  ('env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "71caa2c22f4e8bb7a372824d4ea354f32e50a59e2bbfdcfcd535f56ab45c6d5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@Author     : Ali Mustofa HALOTEC\n",
    "@Module     : Detectiron Container Number\n",
    "@Created on : 29 April 2021\n",
    "'''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import pandas as pd\n",
    "from config import *\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "DIRECTORY_MODEL = 'models'\n",
    "\n",
    "classes = ['container_number']\n",
    "\n",
    "detection_model = {\n",
    "    'filename': 'model_container_number.pt',\n",
    "    'url' : 'https://github.com/Alimustoofaa/JetsonNano-Container_Number/releases/download/1.0/model_container_number.pt',\n",
    "    'file_size' : 19161802\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContainerNumberPrediction:\n",
    "    '''\n",
    "    Load custom model Yolo v5\n",
    "    in directory model/model_container_number.pt\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.model_path = os.path.join(DIRECTORY_MODEL, detection_model['filename'])\n",
    "        self.check_model()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'custom', path_or_model=self.model_path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def check_model(self):\n",
    "        '''\n",
    "        Checking model in model_path\n",
    "        download model if file not found\n",
    "        '''\n",
    "        Path(DIRECTORY_MODEL).mkdir(parents=True, exist_ok=True)\n",
    "        if not os.path.isfile(self.model_path):\n",
    "            print('Downloading container number detection model, please wait.')\n",
    "            response = requests.get(detection_model['url'], stream=True)\n",
    "            progress = tqdm(response.iter_content(1024), \n",
    "                        f'Downloading {detection_model[\"filename\"]}', \n",
    "                        total=detection_model['file_size'], unit='B', \n",
    "                        unit_scale=True, unit_divisor=1024)\n",
    "            with open(self.model_path, 'wb') as f:\n",
    "                for data in progress:\n",
    "                    f.write(data)\n",
    "                    progress.update(len(data))\n",
    "                print('Done downloaded container number detection model.')\n",
    "        else:\n",
    "            print('Load container number detection model.')\n",
    "\n",
    "    def filter_and_crop(self, img, results, min_confidence=0.0):\n",
    "        '''\n",
    "        Format result([tensor([[151.13147, 407.76913, 245.91382, 454.27802,   0.89075,   0.00000]])])\n",
    "        Filter min confidence prediction and classes id/name\n",
    "        Cropped image and get index max value confidence lavel\n",
    "        and retrun image, confidence\n",
    "        '''\n",
    "        confidence_list = list()\n",
    "        image_crop = list()\n",
    "        results_format = results.xyxy\n",
    "        for i in range(len(results_format)):\n",
    "            classes_name = classes[int(results_format[0][i][-1])]\n",
    "            confidence = float(results_format[0][i][-2])\n",
    "            if confidence > min_confidence and classes_name == 'container_number':\n",
    "                x1, y1 = int(results_format[0][i][0]), int(results_format[0][i][1])\n",
    "                x2, y2 = int(results_format[0][i][2]), int(results_format[0][i][3])\n",
    "                crop_image = img[y1-10:y2+10, x1-10:x2+10]\n",
    "                image_crop.append(cropped_img)\n",
    "                confidence_list.append(confidence)\n",
    "                \n",
    "        confidence_max_index = max(range(len(confidence_list)), key=confidence_list.__getitem__)\n",
    "        image_crop_max = image_crop[confidence_max_index]\n",
    "        return image_crop_max, confidence_list[confidence_max_index]\n",
    "\n",
    "    def prediction(self, image):\n",
    "        '''\n",
    "        Prediction image object detectionn YoloV5\n",
    "        output prediction label (xmin,  ymin, xmax, ymax, confidence, class, name)\n",
    "        '''\n",
    "        results = self.model(image)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load container number detection model.\n",
      "Using cache found in /home/ali/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  1    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7255094 parameters, 7255094 gradients\n",
      "\n",
      "Adding autoShape... \n",
      "YOLOv5 ðŸš€ 2021-4-29 torch 1.8.1+cpu CPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_container_number = ContainerNumberPrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('img/1.jpg')\n",
    "results = model_container_number.prediction(image)\n",
    "new_img, confidence = model_container_number.filter_and_crop(img=image, results=results, min_confidence=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 227
    }
   ],
   "source": [
    "cv2.imwrite('test.jpg', new_img)"
   ]
  }
 ]
}